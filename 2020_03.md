### 01 March 2021
  - Week 2 of linear algebra coursera + assignments
  - CS224N: Lecture 2 – Word Vectors and Word Senses

### 02 March 2021
  - Week 3 of linear algebra coursera - gaining radical perspective on meaning of matrices! + assignments (3 hours)
  - Attended Outlander Labs demo day: interesting find - Chip11 (open-source chip designs)
  - CS224N coding assignment 2: Derived word2vec cost, derivatives. Implemented naive softmax cost, SGD. (4 hours)

### 03 March 2021
- Week 4 of linear algebra coursera - Gram Schmidt process, orthogonal matrices, Einstein’s summation method
- assignments.

### 04 March 2021
- CS224N assignment 2 quiz.
- CS224N assignment extra credit.

### 05 March 2021
- Week 5 of linear algebra course: Eigenvalues, vectors - their importance in applying transformations on diagonal matrix.

### 06, 07 March 2021 - weekend

### 08 March 2021
- Week 1 of Multivariate Algebra course
- CS224N - Lecture 3

### 09 March 2021
- CS224N - Lecture 4
Week 2 of Multivariate Algebra course - Jacobian, Hessian

### 10 March 2021
- CS224N - Lecture 5 - Phrase structure vs Dependency Parsing. Dependency parsing history, development, evaluation.

### 11 March 2021
- CS224N - Quiz.
- CS224N - code not complete.

### 12 March 2021
- Code not working
- weekend at Mohonk Mountain.

### 14 March 2021
- Code complete dependency parsing!

### 15 March 2021
- CS224N - Lecture 6
    - Language modeling,
    - RNNs,
    - Evaluation of LM (perplexity)
- CS224N - Lecture 7
    - Vanishing/exploding Gradient in RNNs.
    - LSTM, GRU - mitigate.
    - Very good lecture about minute details in handling gradients.
    - Optimizations: Residual, Dense, and Highway connections.
    - Bidirectional and Multi-layer RNNs.
- CS224N - Lecture 8
    - Machine translation problem,
    - SMT,
    - NMT -- started with seq-to-seq.
    - seq-to-seq consists of Encoder RNN and decoder RNN.
    - BLEU score.
    - Excellent intuition of Attention! Idea: on each step of the decoder, use direct connection to the encoder to focus on a particular part of the source sequence.
    - Generalization of attention.
    - Variants of attention scores: dot-product, multiplicative attention, additive.

### 16 March 2021
- Implemented a NMT model: using bidirectional LSTM for encoding and another unidirectional LSTM for decoding. Spent a lot of time taking care of dimensions of tensors but got stuck with one error which turned out to be application of softmax on a wrong dimension. :D Glad it was sorted out before sleeping though!

### 17 March 2021
- Ran the NMT code on Google Colab - figured out how to use GPUs there.
- CS224N: Quiz 4.

### 18 March 2021
- CS224U - Lecture 1
  What does understanding mean? Three levels of meaning: lexical, compositional, in context. Or representations. Real world state.  
- CS224U - Lecture 2
  - Matrix design,
  - Re-weighing: normalization, observed/expected, *PMI*
  - Dimensionality reduction,
  - Vector comparison : Eucledian, L2, Cosine, matching-based: matching coefficient, Jaccard, dice, overlap; KL divergence.
- CS224N - Lecture 9
  - NMT problems, approaches: more generic discussion on LSTM and GRU.

### 23 March 2021
- CS224N - Lecture 10
  - Question-Answering systems,
  - SQuAD (dataset) - v1 and v2 (half questions dont have answers),
  - Stanford Attentive Reader and Improvements (BiLSTM + Attention and other fine-tuning: expanded input, word-question similarity).
  - BiDAF (Character embedding, Attention flow layer: C2Q and Q2C mapping)
  - FusionNet

- CS224N - Lecture 11
 - Convolutional Networks for NLP
 - Padding, max-pooling (vs avg pooling), stride, local pooling, k-max pooling,
 - dilated convolution (skip rows), deep CNN.
 - Yoon Kim's single layer CNN for classifying text (6 filters -> max-pool -> softmax. 2 word embeddings: keep one static, fine tune the other. Regularization: dropout, constrain L2 norms of weight, etc.)
 - Classification task models: BoW, RNN, CNN, window model.
 - Importance of Residual block and Highway block (shortcut connections).
 - Batch Normalization used with CNNs.
 - Very Deep CNN for text classification: depth 37 works well! (very small compared to images still.)
 - RNNs are slow but carry more history. CNN computations can be parallelized but lose context. So, Quazi-RNNs : can be made to work as well as LSTMs, but faster.
