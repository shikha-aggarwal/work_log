### 06 April 2021
- Understanding Kaggle Coleridge data, went through a few notebooks
- Looking for NLP jobs, communicating.

### 07 April 2021
- Still continuing with scattered reading and coding until I find a good foothold of a problem.
- Read on Question Answering systems:
    - Good introductory post from a couple of years ago: https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507
    - offshoots: CatBoost, ?Boost (Ada, X, ...), InferSent(https://github.com/facebookresearch/InferSent)
    - Google quest challenge on Kaggle: https://www.kaggle.com/c/google-quest-challenge/overview
    - Top solution: https://github.com/oleg-yaroshevskiy/quest_qa_labeling; corresponding post: https://www.kaggle.com/c/google-quest-challenge/discussion/129840
    - Good toolkit: https://github.com/pytorch/fairseq
    - offshoots: Pseudo Labeling, Multi-Sample Dropout (backprop on mean of errors),
    - Started reading BERT paper and realized that I was able to skim through it and get most parts! (Pretty impressive self-education in ~4 months!) offshoots: autoencoders (wiki page): sparse, denoising, contractive, concrete, variational (also GANs): https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368
    https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
    - Attended OpenAI demo day
      - Scaling laws [https://arxiv.org/pdf/2010.14701.pdf]
      - Opinion modeling: temperature/entropy collapse in serialized non-supervied models because of feedback loops.
      - [Jonathan] Large Scale Reward Modeling: Formalize the informal (BLEU, ROUGE), Preference learning (asking people). second: interactive feedback (hire contractors, expensive), internet feedback (gather more data). Internet Feedback: Generic (Twitter, youtube). Task-oriented: reddit. Well-structered. Eg. r/writingprompts. So models: generator, evaluator (response 0, response 1, prompt: 0 or 1), agent model: learns to produce stories and then use feedback to improve.
      pre-trained -> generate -> evaluative and combine last two to make agent.
      rewardmodeling.com
      how well does this reward model generalize?
      - [interesting] Contrastive Language Encoding: data: the pile (Gao)
      Input: two pieces of text that are near to each other as opposed to injecting errors. Prelim results.

### 08 April 2021
- More on autoencoders: https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368
- offshoot: intuition behind MCMC methods: https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50
- Attended Stanford NLP Seminar on Conversational bots. Safety vs engaging. BlenderBot: 2.7 B params, beam search with min length (helps weed out "I don't know" responses).
- Started coding generation using autoencoders.

### 09 April 2021
- Still struggling with Variational autoencoders, read ~10 pages from Carl Doersch's ["Tutorial on Variational Autoencoders"](https://www.researchgate.net/publication/304163568_Tutorial_on_Variational_Autoencoders) but still wasn't clear.
- Almost whole day trying to understand VAEs ... they are so interesting!! Highly recommended source: [Ali Ghodsi, Lec : Deep Learning, Variational Autoencoder, Oct 12 2017](https://www.youtube.com/watch?v=uaaqyVS9-rM)

### 12 April 2021
- Monday spent dealing with job search logistics, talking to people, finding resources for interview prep, and searching the web for more information! Can't complain.

### 13 April 2021
- Upsampling
- Code for autoencoders using Torch.

### 14 April 2021.
- Roberta vs Bert vs DistilBERT: Roberta is Facebook's take on Bert: remove next sentence prediction, longer training, more data, longer sequences, dynamically changing the masking. Good summary: https://www.kdnuggets.com/2019/09/bert-roberta-distilbert-xlnet-one-use.html
DistilBERT is lighter BERT, almost as effective.
- Code for autoencoder.
- Paper on NLI: Compositional Explanations of Neurons
- Gector paper - briefly browsed.
- Started reading Knowledge graph: CS520 https://www.youtube.com/watch?v=bvwjG-3qAmY&list=PLDhh0lALedc7LC_5wpi5gDnPRnu1GSyRG
- Data engineering cookbook: Networks.

### 16 April 2021
- Spacy's NER: Bloom embeddings

### 18 April 2021 [Sunday bonus]
- Tackling the Story Ending Biases in The Story Cloze Test: SCT v1.0 vs v1.5 datasets. Looks like NLP community forgot the original intent of the dataset and started hacking their way around to improve metrics. Even without context, style things like sentiment analysis [VADER and CoreNLP], sentence length and n-grams etc are quite powerful to predice the right ending. And hence improving the dataset.
- NER - good post on NER techniques: https://nanonets.com/blog/named-entity-recognition-2020-guide/
- Broadly we have rule-based, supervised, unsupervised, and deep learning.
- Bi-LSTM with CRF. off-shoot: CRF - conditional random field. Normally, softmax probabilities are independent for each class. But insert CRF to learn relations between output classes. Example in NER, person cannot follow location etc.
- Using BERT for NER: add linear classifier on top.
- NER as Machine Reading Comprehension (MRC): determine indices of entities instead of answers :)

### 19 April 2021
- CS224N: Assignment 5 [WIP]
- LUKE (Language Understanding with Knowledge-based Embeddings): need to explore more.
XLNet (CMU + Google AI): Bigger TransformerXL (20 languages)
- TransformerXL: use previous hidden state for new segment instead of training from scratch (so, passing longer history - XL context) + new positional encoding: "relative position" -- some complicated maths for another time.

### 20 April 2021
- Apples to apples paper: learning through comparisons.
- two leetcode questions
- Gleaner document
- CS224N: Assignment 5 [WIP]!

### 21 April 2021
- Worked on char-based embedding for NMT model [aka assignment 5].

### 22 April 2021
- Trained the model for whole dataset: on colab and Azure.
- Had to restart on colab a couple of times: it is not good for longer training.

### 23 April 2021
- Github and ssh and scp issues!
- Finally submitted the solution with expected BLEU score but another error - which I think is an error on their part. Waiting for their reply.

### 25 April 2021 [Sunday bonus]
- Got the NMT system using char-based BI-LSTM for word embeddings working. BLEU: 16.xx. Need to improve it.

### 26 April 2021
- Leetcode

### 27 April 2021
- Leetcode

### 28 April 2021
- Phone screen - went well
- ML production using Data Foundry (containers). Good video: https://www.youtube.com/watch?v=-UYyyeYJAoQ
- Broadly 3 ways to productionize models: reimplement in Java/C++, PMML, Serialize and deploy Python app.
- Various container management: Amazon ECS vs EKS vs SageMaker.
- Searched and applied to some more positions.
