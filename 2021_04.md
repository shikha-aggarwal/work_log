### 06 April 2021
- Understanding Kaggle Coleridge data, went through a few notebooks
- Looking for NLP jobs, communicating.

### 07 April 2021
- Still continuing with scattered reading and coding until I find a good foothold of a problem.
- Read on Question Answering systems:
    - Good introductory post from a couple of years ago: https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507
    - offshoots: CatBoost, ?Boost (Ada, X, ...), InferSent(https://github.com/facebookresearch/InferSent)
    - Google quest challenge on Kaggle: https://www.kaggle.com/c/google-quest-challenge/overview
    - Top solution: https://github.com/oleg-yaroshevskiy/quest_qa_labeling; corresponding post: https://www.kaggle.com/c/google-quest-challenge/discussion/129840
    - Good toolkit: https://github.com/pytorch/fairseq
    - offshoots: Pseudo Labeling, Multi-Sample Dropout (backprop on mean of errors),
    - Started reading BERT paper and realized that I was able to skim through it and get most parts! (Pretty impressive self-education in ~4 months!) offshoots: autoencoders (wiki page): sparse, denoising, contractive, concrete, variational (also GANs): https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368
    https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
    - Attended OpenAI demo day
      - Scaling laws [https://arxiv.org/pdf/2010.14701.pdf]
      - Opinion modeling: temperature/entropy collapse in serialized non-supervied models because of feedback loops.
      - [Jonathan] Large Scale Reward Modeling: Formalize the informal (BLEU, ROUGE), Preference learning (asking people). second: interactive feedback (hire contractors, expensive), internet feedback (gather more data). Internet Feedback: Generic (Twitter, youtube). Task-oriented: reddit. Well-structered. Eg. r/writingprompts. So models: generator, evaluator (response 0, response 1, prompt: 0 or 1), agent model: learns to produce stories and then use feedback to improve.
      pre-trained -> generate -> evaluative and combine last two to make agent.
      rewardmodeling.com
      how well does this reward model generalize?
      - [interesting] Contrastive Language Encoding: data: the pile (Gao)
      Input: two pieces of text that are near to each other as opposed to injecting errors. Prelim results.

### 08 April 2021
- More on autoencoders: https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368
- offshoot: intuition behind MCMC methods: https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50
- Attended Stanford NLP Seminar on Conversational bots. Safety vs engaging. BlenderBot: 2.7 B params, beam search with min length (helps weed out "I don't know" responses).
- Started coding generation using autoencoders.

### 09 April 2021
- Still struggling with Variational autoencoders, read ~10 pages from Carl Doersch's ["Tutorial on Variational Autoencoders"](https://www.researchgate.net/publication/304163568_Tutorial_on_Variational_Autoencoders) but still wasn't clear.
- Almost whole day trying to understand VAEs ... they are so interesting!! Highly recommended source: [Ali Ghodsi, Lec : Deep Learning, Variational Autoencoder, Oct 12 2017](https://www.youtube.com/watch?v=uaaqyVS9-rM)

### 12 April 2021
- Monday spent dealing with job search logistics, talking to people, finding resources for interview prep, and searching the web for more information! Can't complain.

### 13 April 2021
- Upsampling
- Code for autoencoders using Torch.

### 14 April 2021.
- Roberta vs Bert vs XLNet.
- Code for autoencoder.
- Paper on NLI: Compositional Explanations of Neurons
- Gector paper - briefly browsed.
- Started reading Knowledge graph: CS520 https://www.youtube.com/watch?v=bvwjG-3qAmY&list=PLDhh0lALedc7LC_5wpi5gDnPRnu1GSyRG
- Data engineering cookbook: Networks.
