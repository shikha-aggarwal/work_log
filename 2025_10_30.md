### Temperature and top_p
A transformer model typically spits out a list of many possible next tokens and their individual probabilities. Most of the LLMs don't pick the one with the highest probability because that will make the model too deterministic and same prompt will yield the same answer everytime. Boring. So, there are ways to pick the best next tokens from the list and then pick one.

#### Top_p
This allows the model to sample from the smallest set of tokens whose cumulative probability is more than p. This means a certain number of tokens with highest probabilities.

#### Temperature
The score of each token is scaled before applying softmax (i.e. before computing probability of each). It can be <1, =1, or >1. The score is divided by temperature.

So, if the model is very uncertain => logits are almost equal, then a temperature >1.0 increases that uncertainty even more and makes probabilities closer.

Thought: should we make temperature and top_p adaptive - dependent on how many tokens the model has given us, their distribution, and the kind of output we want - deterministic or very creative(?)
